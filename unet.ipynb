{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f5f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ 正在构建U-Net生成器...\n",
      "\n",
      "⚔️ 模拟进攻开始！虚拟输入图像尺寸: torch.Size([1, 1, 256, 256])\n",
      "\n",
      "============================================================\n",
      "              U-NET 数据流追踪               \n",
      "============================================================\n",
      "▶️ [层级 0] 输入形状: torch.Size([1, 1, 256, 256])\n",
      "  - 正在通过最外层模块...\n",
      "    ▶️ [层级 1] 输入形状: torch.Size([1, 64, 128, 128])\n",
      "        ▶️ [层级 2] 输入形状: torch.Size([1, 128, 64, 64])\n",
      "            ▶️ [层级 3] 输入形状: torch.Size([1, 256, 32, 32])\n",
      "                ▶️ [层级 4] 输入形状: torch.Size([1, 512, 16, 16])\n",
      "                    ▶️ [层级 5] 输入形状: torch.Size([1, 512, 8, 8])\n",
      "                        ▶️ [层级 6] 输入形状: torch.Size([1, 512, 4, 4])\n",
      "                            ▶️ [层级 7] 输入形状: torch.Size([1, 512, 2, 2])\n",
      "                              - U型子结构输出形状: torch.Size([1, 512, 2, 2])\n",
      "                              - 准备拼接 (Concat):\n",
      "                                - 编码器援军 (x)      : torch.Size([1, 512, 2, 2])\n",
      "                                - 解码器主力 (sub_output): torch.Size([1, 512, 2, 2])\n",
      "                              - 拼接后形状: torch.Size([1, 1024, 2, 2]) (通道数 = 512 + 512)\n",
      "                            ◀️ [层级 7] 返回形状: torch.Size([1, 1024, 2, 2])\n",
      "                          - U型子结构输出形状: torch.Size([1, 512, 4, 4])\n",
      "                          - 准备拼接 (Concat):\n",
      "                            - 编码器援军 (x)      : torch.Size([1, 512, 4, 4])\n",
      "                            - 解码器主力 (sub_output): torch.Size([1, 512, 4, 4])\n",
      "                          - 拼接后形状: torch.Size([1, 1024, 4, 4]) (通道数 = 512 + 512)\n",
      "                        ◀️ [层级 6] 返回形状: torch.Size([1, 1024, 4, 4])\n",
      "                      - U型子结构输出形状: torch.Size([1, 512, 8, 8])\n",
      "                      - 准备拼接 (Concat):\n",
      "                        - 编码器援军 (x)      : torch.Size([1, 512, 8, 8])\n",
      "                        - 解码器主力 (sub_output): torch.Size([1, 512, 8, 8])\n",
      "                      - 拼接后形状: torch.Size([1, 1024, 8, 8]) (通道数 = 512 + 512)\n",
      "                    ◀️ [层级 5] 返回形状: torch.Size([1, 1024, 8, 8])\n",
      "                  - U型子结构输出形状: torch.Size([1, 512, 16, 16])\n",
      "                  - 准备拼接 (Concat):\n",
      "                    - 编码器援军 (x)      : torch.Size([1, 512, 16, 16])\n",
      "                    - 解码器主力 (sub_output): torch.Size([1, 512, 16, 16])\n",
      "                  - 拼接后形状: torch.Size([1, 1024, 16, 16]) (通道数 = 512 + 512)\n",
      "                ◀️ [层级 4] 返回形状: torch.Size([1, 1024, 16, 16])\n",
      "              - U型子结构输出形状: torch.Size([1, 256, 32, 32])\n",
      "              - 准备拼接 (Concat):\n",
      "                - 编码器援军 (x)      : torch.Size([1, 256, 32, 32])\n",
      "                - 解码器主力 (sub_output): torch.Size([1, 256, 32, 32])\n",
      "              - 拼接后形状: torch.Size([1, 512, 32, 32]) (通道数 = 256 + 256)\n",
      "            ◀️ [层级 3] 返回形状: torch.Size([1, 512, 32, 32])\n",
      "          - U型子结构输出形状: torch.Size([1, 128, 64, 64])\n",
      "          - 准备拼接 (Concat):\n",
      "            - 编码器援军 (x)      : torch.Size([1, 128, 64, 64])\n",
      "            - 解码器主力 (sub_output): torch.Size([1, 128, 64, 64])\n",
      "          - 拼接后形状: torch.Size([1, 256, 64, 64]) (通道数 = 128 + 128)\n",
      "        ◀️ [层级 2] 返回形状: torch.Size([1, 256, 64, 64])\n",
      "      - U型子结构输出形状: torch.Size([1, 64, 128, 128])\n",
      "      - 准备拼接 (Concat):\n",
      "        - 编码器援军 (x)      : torch.Size([1, 64, 128, 128])\n",
      "        - 解码器主力 (sub_output): torch.Size([1, 64, 128, 128])\n",
      "      - 拼接后形状: torch.Size([1, 128, 128, 128]) (通道数 = 64 + 64)\n",
      "    ◀️ [层级 1] 返回形状: torch.Size([1, 128, 128, 128])\n",
      "◀️ [层级 0] 输出形状: torch.Size([1, 1, 256, 256])\n",
      "============================================================\n",
      "\n",
      "✅ 凯旋！最终输出图像尺寸: torch.Size([1, 1, 256, 256])\n",
      "👍 验证成功：输出尺寸与输入尺寸完全一致。\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 👑 U-Net 生成器形状变换：沙盘推演\n",
    "# \n",
    "# ## 🎯 本次推演目标\n",
    "# 1.  **可视化追踪**：亲眼见证一个输入张量在您的`UnetGenerator`中，从输入到输出的完整形状变换旅程。\n",
    "# 2.  **解密递归**：彻底理解`UnetSkipConnectionBlock`的递归嵌套是如何工作的。\n",
    "# 3.  **洞悉拼接**：精确掌握`torch.cat`（跳跃连接）在每一层是如何改变通道数的。\n",
    "# \n",
    "# **我们将完全基于您代码库中的`network.py`和默认参数（`num_downs=8`）进行推演。**\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 🛠️ 第一步：复刻兵器库 - 从`network.py`中提取核心组件\n",
    "# \n",
    "# 我们首先将`UnetGenerator`和其核心“积木块”`UnetSkipConnectionBlock`的代码复制到这里。为了追踪形状，我们将在`UnetSkipConnectionBlock`的`forward`方法中加入打印语句，作为我们的“侦察探针”。\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 这是 UnetSkipConnectionBlock 的“侦察兵”版本\n",
    "# 我们在 forward 方法中加入了详细的打印语句\n",
    "class InstrumentedUnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "    X -------------------identity----------------------\n",
    "    |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False, level=0):\n",
    "        super(InstrumentedUnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        # 核心：记录当前积木块的层级，便于打印\n",
    "        self.level = level\n",
    "        self.indent = \"    \" * level\n",
    "        \n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "            \n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=False)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = [\n",
    "                *down,\n",
    "                submodule,\n",
    "                *up\n",
    "            ]\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=False)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = [\n",
    "                *down,\n",
    "                *up\n",
    "            ]\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=False)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = [*down, submodule, *up, nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = [*down, submodule, *up]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"{self.indent}▶️ [层级 {self.level}] 输入形状: {x.shape}\")\n",
    "        \n",
    "        # 如果不是最外层，则执行跳跃连接\n",
    "        if self.outermost:\n",
    "            print(f\"{self.indent}  - 正在通过最外层模块...\")\n",
    "            output = self.model(x)\n",
    "            print(f\"{self.indent}◀️ [层级 {self.level}] 输出形状: {output.shape}\")\n",
    "            return output\n",
    "        else:\n",
    "            # self.model(x) 是数据走完一个U型子结构（先下后上）的结果\n",
    "            sub_output = self.model(x)\n",
    "            print(f\"{self.indent}  - U型子结构输出形状: {sub_output.shape}\")\n",
    "            \n",
    "            # torch.cat 是跳跃连接的核心\n",
    "            print(f\"{self.indent}  - 准备拼接 (Concat):\")\n",
    "            print(f\"{self.indent}    - 编码器援军 (x)      : {x.shape}\")\n",
    "            print(f\"{self.indent}    - 解码器主力 (sub_output): {sub_output.shape}\")\n",
    "            \n",
    "            result = torch.cat([x, sub_output], 1)\n",
    "            print(f\"{self.indent}  - 拼接后形状: {result.shape} (通道数 = {x.shape[1]} + {sub_output.shape[1]})\")\n",
    "            print(f\"{self.indent}◀️ [层级 {self.level}] 返回形状: {result.shape}\")\n",
    "            return result\n",
    "\n",
    "# 这是 UnetGenerator 的修改版，它使用我们带“探针”的积木块\n",
    "class InstrumentedUnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(InstrumentedUnetGenerator, self).__init__()\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = InstrumentedUnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True, level=num_downs-1)\n",
    "        for i in range(num_downs - 5):\n",
    "            unet_block = InstrumentedUnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout, level=num_downs-2-i)\n",
    "        unet_block = InstrumentedUnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, level=3)\n",
    "        unet_block = InstrumentedUnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer, level=2)\n",
    "        unet_block = InstrumentedUnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer, level=1)\n",
    "        self.model = InstrumentedUnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer, level=0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ⚔️ 第二步：沙盘推演 - 发起一次模拟进攻\n",
    "# \n",
    "# 现在，我们将创建一个虚拟的输入图像，并将其送入我们装备了“探针”的U-Net生成器。请仔细观察下方打印的输出，它会清晰地展示数据是如何逐层下沉，又如何逐层上浮并融合的。\n",
    "\n",
    "# %%\n",
    "# --- 参数设定 (与您代码默认值一致) ---\n",
    "INPUT_CHANNELS = 1     # 输入通道数 (灰度图)\n",
    "OUTPUT_CHANNELS = 1    # 输出通道数 (灰度图)\n",
    "NUM_DOWNS = 8          # 下采样总层数\n",
    "NGF = 64               # 基础滤波器数量\n",
    "\n",
    "# --- 实例化我们带“探针”的生成器 ---\n",
    "print(\"🏗️ 正在构建U-Net生成器...\\n\")\n",
    "generator = InstrumentedUnetGenerator(INPUT_CHANNELS, OUTPUT_CHANNELS, NUM_DOWNS, ngf=NGF)\n",
    "\n",
    "# --- 创建一个虚拟输入张量 ---\n",
    "# (批量, 通道, 高, 宽)\n",
    "# 注意：由于有8次下采样，输入图像的宽高必须是 2^8 = 256 的倍数\n",
    "BATCH_SIZE = 1\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "dummy_input = torch.randn(BATCH_SIZE, INPUT_CHANNELS, HEIGHT, WIDTH)\n",
    "\n",
    "print(f\"⚔️ 模拟进攻开始！虚拟输入图像尺寸: {dummy_input.shape}\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"              U-NET 数据流追踪               \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- 发起进攻！---\n",
    "output = generator(dummy_input)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✅ 凯旋！最终输出图像尺寸: {output.shape}\")\n",
    "\n",
    "# --- 验证 ---\n",
    "assert output.shape == dummy_input.shape\n",
    "print(\"👍 验证成功：输出尺寸与输入尺寸完全一致。\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 📜 第三步：战情报告解读\n",
    "# \n",
    "# 请您仔细阅读并分析上方单元格的输出。您会发现：\n",
    "# \n",
    "# 1.  **层级关系**：打印输出的缩进清晰地展示了网络的递归调用关系。`[层级 0]` 是最外层，`[层级 7]` 是最内层的瓶颈。\n",
    "# 2.  **编码器（下采样）**：观察从 `[层级 0]` 到 `[层级 7]` 的`输入形状`，您会看到空间尺寸（高和宽）是如何一步步减半的 (`256` -> `128` -> ... -> `1`)。\n",
    "# 3.  **解码器（上采样）**：观察从 `[层级 7]` 开始的返回过程。在每一个层级，`U型子结构输出形状`的空间尺寸，都是其`输入形状`的一半。\n",
    "# 4.  **跳跃连接**：在每一个层级（除最外层），观察`准备拼接 (Concat)`部分。您会看到“编码器援军”和“解码器主力”的空间尺寸是完全相同的，它们的通道数相加，形成了`拼接后形状`的总通道数。\n",
    "# \n",
    "# 这份 Notebook 就是您勘察 U-Net 内部结构的动态地图。请反复运行、修改参数（如`NUM_DOWNS`）并观察其变化，直到您对这股数据洪流的走向了然于胸。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augan38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
